# Distributed Web Crawler using Dask

## ğŸ“Œ Project Description
This project implements a Distributed Web Crawler using Dask to crawl multiple web pages in parallel. It extracts meaningful text content and performs word-frequency analysis to analyze website data efficiently.

## ğŸ›  Tools & Technologies
- Python
- Dask
- BeautifulSoup
- Requests
- Matplotlib

## ğŸ“‚ Project Components
- Crawler script / Jupyter Notebook
- Word-frequency analysis
- Comparison plots
- Project report
- Presentation slides

## âš™ï¸ Project Workflow
1. Studied Dask basics for parallel computing
2. Implemented crawling functions using BeautifulSoup
3. Distributed crawling using Dask Bags
4. Cleaned extracted text data
5. Performed word frequency analysis
6. Generated comparison plots
7. Tested the full pipeline and documented results

## ğŸ“Š Outputs
- Crawled website text
- Word frequency plots
- Comparative analysis graphs

## âœ… Conclusion
This project demonstrates effective use of distributed computing to improve web crawling performance compared to traditional sequential approaches.

## ğŸ‘©â€ğŸ’» Author
Hina Naheem  
BS Computer Science (8th Semester)
